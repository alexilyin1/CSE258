{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Alexander Ilyin CSE 258"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy \n",
    "import warnings\n",
    "import gzip\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        yield l.strip().split(',')\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "data = pd.read_csv('train_Interactions.csv.gz')\n",
    "for i in range(len(data)):\n",
    "    X.extend([[data['userID'][i], data['bookID'][i]]])\n",
    "    y.append(1)\n",
    "    \n",
    "X_train = X[:190000]\n",
    "y_train = y[:190000]\n",
    "X_val = X[190000:200000]\n",
    "y_val = y[190000:200000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Create \"negative\" entries for books that users haven't read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "all_books = set([x[1] for x in X])\n",
    "\n",
    "for x in X:\n",
    "    u, i = x[0], x[1]\n",
    "    usersPerItem[i].add(u)\n",
    "    itemsPerUser[u].add(i)\n",
    "    \n",
    "for x in range(len(X_val)):\n",
    "    user = X_val[x][0]\n",
    "    unread_books = list(all_books.difference(itemsPerUser[user]))\n",
    "    choice = unread_books[random.randint(0, len(unread_books)-1)]\n",
    "    X_val.extend([[X_val[x][0], choice]])\n",
    "    y_val.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for x in range(len(X_train)):\n",
    "    bookCount[X_train[x][1]] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalRead/2: break\n",
    "\n",
    "predictions = open(\"predictions_Read_question1.txt\", 'w')\n",
    "for l in range(len(X_val)):\n",
    "    u = X_val[l][0]\n",
    "    b = X_val[l][1]\n",
    "    if b in return1:\n",
    "        predictions.write(u + '-' + b + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "\n",
    "predictions.close()\n",
    "\n",
    "predictions = open(\"predictions_Read_question1.txt\", \"r\")\n",
    "predictions_array = []\n",
    "for l in predictions.readlines():\n",
    "    predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "y_actual = []\n",
    "y_actual = numpy.append(y_actual, y_val)\n",
    "correctPredictions = 0\n",
    "\n",
    "for x in range(len(predictions_array)):\n",
    "    if numpy.array_equal(predictions_array[x], str(int(y_actual[x]))):\n",
    "        correctPredictions = correctPredictions + 1\n",
    "\n",
    "accuracy = correctPredictions/len(predictions_array)\n",
    "print(f'The accuracy of the baseline model is {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Improve Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for x in range(len(X)):\n",
    "    bookCount[X[x][1]] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "threshold_list = []\n",
    "accuracy_list = []\n",
    "for threshold in range(0, totalRead, 10000):\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > threshold : break\n",
    "\n",
    "    predictions = open(\"predictions_Read_question2.txt\", 'w')\n",
    "    for l in range(len(X_val)):\n",
    "        u = X_val[l][0]\n",
    "        b = X_val[l][1]\n",
    "        if b in return1:\n",
    "            predictions.write(u + '-' + b + \",1\\n\")\n",
    "        else:\n",
    "            predictions.write(u + '-' + b + \",0\\n\")\n",
    "\n",
    "    predictions.close()\n",
    "\n",
    "    predictions = open(\"predictions_Read_question2.txt\", \"r\")\n",
    "    predictions_array = []\n",
    "    for l in predictions.readlines():\n",
    "        l.strip('\\n')\n",
    "        predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "    y_actual = []\n",
    "    y_actual = numpy.append(y_actual, y_val)\n",
    "    correctPredictions = 0\n",
    "\n",
    "    for x in range(len(predictions_array)):\n",
    "        if predictions_array[x] == str(int(y_actual[x])):\n",
    "            correctPredictions += 1\n",
    "    \n",
    "    threshold_list.append(threshold)\n",
    "    accuracy = correctPredictions/len(predictions_array)\n",
    "    accuracy_list.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_threshold = pd.DataFrame(list(zip(threshold_list, accuracy_list)), \n",
    "                                  columns=[\"threshold\", \"accuracy\"])\n",
    "plt.plot('threshold','accuracy', data=accuracy_threshold)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_threshold.loc[accuracy_threshold['accuracy'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > 114500: break\n",
    "\n",
    "predictions = open(\"predictions_Read_question2.txt\", 'w')\n",
    "for l in range(len(X_val)):\n",
    "    u = X_val[l][0]\n",
    "    b = X_val[l][1]\n",
    "    if b in return1:\n",
    "        predictions.write(u + '-' + b + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "\n",
    "predictions.close()\n",
    "\n",
    "predictions = open(\"predictions_Read_question2.txt\", \"r\")\n",
    "predictions_array = []\n",
    "for l in predictions.readlines():\n",
    "    l.strip('\\n')\n",
    "    predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "y_actual = []\n",
    "y_actual = numpy.append(y_actual, y_val)\n",
    "correctPredictions = 0\n",
    "\n",
    "for x in range(len(predictions_array)):\n",
    "    if predictions_array[x] == str(int(y_actual[x])):\n",
    "        correctPredictions += 1\n",
    "        \n",
    "print(correctPredictions/len(predictions_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the code snippet above, we can see that the threshold of 114,500 gives us the best accuracy, based on the parameter tuning pipeline created above\n",
    "Note: Does not match the above value, this is because I ran a code that took a very long time to run and simply recorded the answer, I then run a smaller loop to make re-running the code faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for x in range(len(X)):\n",
    "    bookCount[X[x][1]] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > 114500 : break\n",
    "\n",
    "predictions = open(\"predictions_Read_kaggleq1.txt\", 'w')\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    if b in return1:\n",
    "        predictions.write(u + '-' + b + \",1\\n\")\n",
    "    else: \n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "\n",
    "for x in X_train:\n",
    "    u, i = x[0], x[1]\n",
    "    usersPerItem[i].add(u)\n",
    "    itemsPerUser[u].add(i)\n",
    "    \n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer/denom\n",
    "\n",
    "def mostSimilar(u, b):\n",
    "    similarities = []\n",
    "    books = set(itemsPerUser[u])\n",
    "    for book in books:\n",
    "        sim = Jaccard(usersPerItem[b], usersPerItem[book])\n",
    "        similarities.append((sim,book))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:10]\n",
    "\n",
    "predictions = open(\"predictions_Read_question3.txt\", 'w')\n",
    "for l in range(len(X_val)):\n",
    "    if mostSimilar(X_val[l][0], X_val[l][1]):\n",
    "        if max(mostSimilar(X_val[l][0], X_val[l][1]))[0] > 0.008:\n",
    "            predictions.write(X_val[l][0] + '-' + X_val[l][1] + \",1\\n\")\n",
    "        else:\n",
    "            predictions.write(X_val[l][0] + '-' + X_val[l][1] + \",0\\n\")\n",
    "    else: \n",
    "        predictions.write(X_val[l][0] + '-' + X_val[l][1] + \",0\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Jaccard similarity baseline model is 0.4963\n"
     ]
    }
   ],
   "source": [
    "predictions = open(\"predictions_Read_question3.txt\", \"r\")\n",
    "predictions_array = []\n",
    "for l in predictions.readlines():\n",
    "    l.strip('\\n')\n",
    "    predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "y_actual = []\n",
    "y_actual = numpy.append(y_actual, y_val)\n",
    "correctPredictions = 0\n",
    "\n",
    "for x in range(len(predictions_array)):\n",
    "    if predictions_array[x] == str(int(y_actual[x])):\n",
    "        correctPredictions += 1\n",
    "\n",
    "accuracy = correctPredictions/len(predictions_array)\n",
    "print(f'The accuracy of the Jaccard similarity baseline model is {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "\n",
    "for x in X_train:\n",
    "    u, i = x[0], x[1]\n",
    "    usersPerItem[i].add(u)\n",
    "    itemsPerUser[u].add(i)\n",
    "    \n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer/denom\n",
    "\n",
    "def mostSimilar(u, b):\n",
    "    similarities = []\n",
    "    books = set(itemsPerUser[u])\n",
    "    for book in books:\n",
    "        sim = Jaccard(usersPerItem[b], usersPerItem[book])\n",
    "        similarities.append((sim,book))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:10]\n",
    "\n",
    "threshold_list = []\n",
    "accuracy_list = []\n",
    "for threshold in numpy.arange(0, .05, .005):\n",
    "    predictions = open(\"predictions_Read_question3.txt\", 'w')\n",
    "    for l in range(len(X_val)):\n",
    "        if mostSimilar(X_val[l][0], X_val[l][1]):\n",
    "            if max(mostSimilar(X_val[l][0], X_val[l][1]))[0] > threshold:\n",
    "                predictions.write(X_val[l][0] + '-' + X_val[l][1] + \",1\\n\")\n",
    "            else:\n",
    "                predictions.write(X_val[l][0] + '-' + X_val[l][1] + \",0\\n\")\n",
    "        else: \n",
    "            predictions.write(X_val[l][0] + '-' + X_val[l][1] + \",0\\n\")\n",
    "    predictions.close()\n",
    "    predictions = open(\"predictions_Read_question3.txt\", \"r\")\n",
    "    predictions_array = []\n",
    "    for l in predictions.readlines():\n",
    "        l.strip('\\n')\n",
    "        predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "    y_actual = []\n",
    "    y_actual = numpy.append(y_actual, y_val)\n",
    "    correctPredictions = 0\n",
    "\n",
    "    for x in range(len(predictions_array)):\n",
    "        if predictions_array[x] == str(int(y_actual[x])):\n",
    "            correctPredictions += 1\n",
    "    \n",
    "    threshold_list.append(threshold)\n",
    "    accuracy_list.append(accuracy)\n",
    "    accuracy = correctPredictions/len(predictions_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_accuracy_jaccard = pd.DataFrame(list(zip(threshold_list, accuracy_list)),\n",
    "                                          columns=['threshold', 'accuracy'])\n",
    "plt.plot('threshold','accuracy', data=threshold_accuracy_jaccard)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_accuracy_jaccard.loc[threshold_accuracy_jaccard['accuracy'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code chunk above tells us that the Jaccard similarity threshold of .0120 gives us the best accuracy. Note that this may not match the printed value above, as I changed the loop to a shorter run time. The value above was calculated from a longer, more exact loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "\n",
    "for x in X_train:\n",
    "    u, i = x[0], x[1]\n",
    "    usersPerItem[i].add(u)\n",
    "    itemsPerUser[u].add(i)\n",
    "    \n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer/denom\n",
    "\n",
    "def mostSimilar(u, b):\n",
    "    similarities = []\n",
    "    books = set(itemsPerUser[u])\n",
    "    for book in books:\n",
    "        sim = Jaccard(usersPerItem[b], usersPerItem[book])\n",
    "        similarities.append((sim,book))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:10]\n",
    "\n",
    "predictions = open(\"predictions_Read_kaggleq3.txt\", 'w')\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    if mostSimilar(u, b):\n",
    "        if max(mostSimilar(u, b))[0] > 0.0120:\n",
    "            predictions.write(u + '-' + b + \",1\\n\")\n",
    "        else:\n",
    "            predictions.write(u + '-' + b + \",0\\n\")\n",
    "    else: \n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Implementing Jaccard and Popularity Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for user,book in X:\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > 120000 : break\n",
    "        \n",
    "predictions = open(\"predictions_Read_question4.txt\", 'w')\n",
    "for l in range(len(X_val)):\n",
    "    u = X_val[l][0]\n",
    "    b = X_val[l][1]\n",
    "    if mostSimilar(X_val[l][0], X_val[l][1]):\n",
    "        if b in return1 or max(mostSimilar(X_val[l][0], X_val[l][1]))[0] > 0.0120:\n",
    "            predictions.write(u + '-' + b + \",1\\n\")\n",
    "        else:\n",
    "            predictions.write(u + '-' + b + \",0\\n\")\n",
    "    elif b in return1:\n",
    "        predictions.write(u + '-' + b + \",1\\n\")\n",
    "    else: \n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Read_question4.txt\", \"r\")\n",
    "predictions_array = []\n",
    "for l in predictions.readlines():\n",
    "    l.strip('\\n')\n",
    "    predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "y_actual = []\n",
    "y_actual = numpy.append(y_actual, y_val)\n",
    "correctPredictions = 0\n",
    "\n",
    "for x in range(len(predictions_array)):\n",
    "    if predictions_array[x] == str(int(y_actual[x])):\n",
    "        correctPredictions += 1\n",
    "\n",
    "accuracy = correctPredictions/len(predictions_array)\n",
    "print(f'The accuracy of the Jaccard and popularity baseline model is {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: Making Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for user,book in X:\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "\n",
    "threshold_popular1_list = []\n",
    "threshold_jaccard1_list = []\n",
    "accuracy1_list = []\n",
    "for threshold_popular1 in range(100000, totalRead, 1000):\n",
    "    for threshold_jaccard1 in numpy.arange(0.001, 0.01, .0010):\n",
    "        \n",
    "        return1 = set()\n",
    "        count = 0\n",
    "        for ic, i in mostPopular:\n",
    "            count += ic\n",
    "            return1.add(i)\n",
    "            if count > threshold_popular1 : break\n",
    "                \n",
    "        predictions = open(\"predictions_Read_question4.txt\", 'w')\n",
    "        for l in range(len(X_val)):\n",
    "            u = X_val[l][0]\n",
    "            b = X_val[l][1]\n",
    "            if mostSimilar(X_val[l][0], X_val[l][1]):\n",
    "                if b in return1 and max(mostSimilar(X_val[l][0], X_val[l][1]))[0] > threshold_jaccard1:\n",
    "                    predictions.write(u + '-' + b + \",1\\n\")\n",
    "                else:\n",
    "                    predictions.write(u + '-' + b + \",0\\n\")\n",
    "            elif b in return1:\n",
    "                predictions.write(u + '-' + b + \",1\\n\")\n",
    "            else: \n",
    "                predictions.write(u + '-' + b + \",0\\n\")\n",
    "        predictions.close()\n",
    "\n",
    "        predictions = open(\"predictions_Read_question4.txt\", \"r\")\n",
    "        predictions_array = []\n",
    "        for l in predictions.readlines():\n",
    "            l.strip('\\n')\n",
    "            predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "        y_actual = []\n",
    "        y_actual = numpy.append(y_actual, y_val)\n",
    "        correctPredictions = 0\n",
    "\n",
    "        for x in range(len(predictions_array)):\n",
    "            if predictions_array[x] == str(int(y_actual[x])):\n",
    "                correctPredictions += 1\n",
    "\n",
    "        accuracy = correctPredictions/len(predictions_array)\n",
    "        threshold_popular1_list.append(threshold_popular1)\n",
    "        threshold_jaccard1_list.append(threshold_jaccard1)\n",
    "        accuracy1_list.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_model_df = pd.DataFrame(list(zip(threshold_popular1_list, threshold_jaccard1_list, accuracy1_list)), columns=['Popularity Threshold', 'Jaccard Threshold', 'Accuracy'])\n",
    "combo_model_df.loc[combo_model_df['Accuracy'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above, I changed the length of the loop in order to reduce reporducing runtime. The values I chose for my final solution are not represented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for x in range(len(X)):\n",
    "    bookCount[X[x][1]] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "accuracy_list = []\n",
    "threshold_popular_list = []\n",
    "threshold_jaccard_list = []\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > 132000: break\n",
    "\n",
    "predictions = open(\"predictions_Read_kaggleq5.txt\", 'w')\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    if mostSimilar(u, b):\n",
    "        if b in return1 and max(mostSimilar(u, b))[0] > 0.0080:\n",
    "            predictions.write(u + '-' + b + \",1\\n\")\n",
    "        else:\n",
    "            predictions.write(u + '-' + b + \",0\\n\")\n",
    "    elif b in return1:\n",
    "        predictions.write(u + '-' + b + \",1\\n\")\n",
    "    else: \n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Kaggle username/display name is \"Alexander Ilyin\", I signed up with email \"ailyin@ucsd.edu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Fitting Simple Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from statistics import mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "itemsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(list)\n",
    "user_ratings_train = defaultdict(list)\n",
    "user_averages_train = defaultdict(dict)\n",
    "items_ratings_train = defaultdict(list)\n",
    "items_averages_train = defaultdict(dict)\n",
    "user_biases = defaultdict(list)\n",
    "item_biases = defaultdict(list)\n",
    "\n",
    "for user,book,r in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    X.extend([[user, book]])\n",
    "    r = int(r)\n",
    "    y.append(r)\n",
    "    \n",
    "X_train = X[:190000]\n",
    "y_train = y[:190000]\n",
    "X_val = X[190000:200000]\n",
    "y_val = y[190000:200000]\n",
    "\n",
    "for x in range(len(X_train)):\n",
    "    itemsPerUser[X_train[x][0]].append(X_train[x][1])\n",
    "    usersPerItem[X_train[x][1]].append(X_train[x][0])\n",
    "    user_ratings_train[X_train[x][0]].append(y_train[x])\n",
    "    items_ratings_train[X_train[x][1]].append(y_train[x])\n",
    "\n",
    "for u in user_ratings_train:\n",
    "    user_averages_train[u] = sum(user_ratings_train[u]) / len(user_ratings_train[u])\n",
    "    \n",
    "for i in items_ratings_train:\n",
    "    items_averages_train[i] = sum(items_ratings_train[i]) / len(items_ratings_train[i])\n",
    "    \n",
    "global_avg = mean(y_train)\n",
    "\n",
    "for u in user_averages_train:\n",
    "    user_biases[u] = user_averages_train[u] - global_avg\n",
    "for i in items_averages_train:\n",
    "    item_biases[i] = items_averages_train[i] - global_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error on the training set is: 0.9329868496220741\n",
      "The mean squared error on the training set is: 0.9305914382658055\n",
      "The mean squared error on the training set is: 0.9296826090859084\n",
      "The mean squared error on the training set is: 0.9290562032960668\n",
      "The mean squared error on the training set is: 0.9284935908286799\n",
      "The mean squared error on the training set is: 0.9279362948813408\n",
      "The mean squared error on the training set is: 0.9273659114016267\n",
      "The mean squared error on the training set is: 0.9267770003521404\n",
      "The mean squared error on the training set is: 0.9261691373513172\n",
      "The mean squared error on the training set is: 0.9255440639979667\n",
      "The mean squared error on the validation set is: 1.1439179356396383\n"
     ]
    }
   ],
   "source": [
    "def pred(a,b,b1):\n",
    "    return a + b + b1\n",
    "\n",
    "def MSE(actual:list, pred:list):\n",
    "    summ = 0\n",
    "    for x in range(len(actual)):\n",
    "        summ += (float(actual[x] - pred[x]))**2\n",
    "    summ = summ / len(actual)\n",
    "    return summ\n",
    "old_user_bias = defaultdict(list)\n",
    "old_item_bias = defaultdict(list)\n",
    "counter = 10\n",
    "while counter > 0:\n",
    "    summ = 0 \n",
    "    for x in range(len(X_train)):\n",
    "        summ += y_train[x] - (user_biases[X_train[x][0]]+item_biases[X_train[x][1]])\n",
    "    alpha = summ / len(y_train)\n",
    "    y_pred = []     \n",
    "    for user in itemsPerUser:\n",
    "        summ_b = 0\n",
    "        for i in range(len(itemsPerUser[user])):\n",
    "            summ_b += user_ratings_train[user][i] - (alpha + item_biases[itemsPerUser[user][i]])\n",
    "        beta_u = (summ_b) / (1+len(user_ratings_train[user]))\n",
    "        user_bias_new[user] = beta_u\n",
    "        user_biases[user] = beta_u\n",
    "    for book in usersPerItem:\n",
    "        summ_b1 = 0\n",
    "        for i in range(len(usersPerItem[book])):\n",
    "            summ_b1 += items_ratings_train[book][i] - (alpha + user_biases[usersPerItem[book][i]])\n",
    "        beta_i = (summ_b1) / (1+len(items_ratings_train[book]))\n",
    "        item_bias_new[book] = beta_i \n",
    "        item_biases[book] = beta_i\n",
    "        \n",
    "    for user, book in X_train:\n",
    "        prediction = pred(alpha, item_bias_new[book], user_bias_new[user])\n",
    "        y_pred.append(prediction)\n",
    "    mse = MSE(y_train, y_pred)\n",
    "    print(f\"The mean squared error on the training set is: {mse}\")\n",
    "    counter = counter - 1\n",
    "\n",
    "y_pred = []\n",
    "for user, item in X_val:\n",
    "    if user_bias_new[user] != [] and item_bias_new[item] != []:\n",
    "        predictions = pred(alpha, user_bias_new[user], item_bias_new[item])\n",
    "        y_pred.append(predictions)\n",
    "    elif user_bias_new[user] == []:\n",
    "        predictions = pred(alpha, 0, item_bias_new[item])\n",
    "        y_pred.append(predictions)\n",
    "    elif item_bias_new[item] == []:\n",
    "        predictions = pred(alpha, user_bias_new[user], 0)\n",
    "        y_pred.append(predictions)\n",
    "mse = MSE(y_val, y_pred)\n",
    "print(f\"The mean squared error on the validation set is: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: Largest and Smallest beta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larget user beta = u06559157 and smallest user beta = u48313610\n",
      "Largest item beta = b19925500 and smallest item beta = b21479253\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "item_bias_new['b21479253'] = 0.0\n",
    "\n",
    "print(f\"Larget user beta = {max(user_bias_new.items(), key=operator.itemgetter(1))[0]} and smallest user beta = {min(user_bias_new.items(), key=operator.itemgetter(1))[0]}\")\n",
    "print(f\"Largest item beta = {max(item_bias_new.items(), key=operator.itemgetter(1))[0]} and smallest item beta = {min(item_bias_new.items(), key=operator.itemgetter(1))[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11: Finding an optimal lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error on the validation set is: 1.135032165320977\n",
      "The mean squared error on the validation set is: 1.1350132534554138\n",
      "The mean squared error on the validation set is: 1.1166362321337446\n",
      "The mean squared error on the validation set is: 1.1095156247115663\n",
      "The mean squared error on the validation set is: 1.1109753717032733\n",
      "The mean squared error on the validation set is: 1.114467931487997\n",
      "The mean squared error on the validation set is: 1.1186094881368513\n",
      "The mean squared error on the validation set is: 1.123167221010631\n",
      "The mean squared error on the validation set is: 1.1280004941405632\n",
      "The mean squared error on the validation set is: 1.13299828206863\n",
      "The mean squared error on the validation set is: 1.1380790197101247\n",
      "The mean squared error on the validation set is: 1.1431841926382649\n",
      "The mean squared error on the validation set is: 1.1482719334651406\n",
      "The mean squared error on the validation set is: 1.1533123203772213\n",
      "The mean squared error on the validation set is: 1.1582840820940612\n",
      "The mean squared error on the validation set is: 1.1631722739249672\n",
      "The mean squared error on the validation set is: 1.1679666161429774\n",
      "The mean squared error on the validation set is: 1.172660290407747\n",
      "The mean squared error on the validation set is: 1.177249057984967\n"
     ]
    }
   ],
   "source": [
    "mse_list = []\n",
    "lambda_list = []\n",
    "\n",
    "for l in range(1, 20):\n",
    "    counter = 10\n",
    "    user_bias_new = defaultdict(list)\n",
    "    item_bias_new = defaultdict(list)\n",
    "    old_user_bias = defaultdict(list)\n",
    "    old_item_bias = defaultdict(list)\n",
    "    counter = 10\n",
    "    while counter > 0:\n",
    "        summ = 0 \n",
    "        for x in range(len(X_train)):\n",
    "            summ += y_train[x] - (user_biases[X_train[x][0]]+item_biases[X_train[x][1]])\n",
    "        alpha = summ / len(y_train)\n",
    "        y_pred = []     \n",
    "        for user in itemsPerUser:\n",
    "            summ_b = 0\n",
    "            for i in range(len(itemsPerUser[user])):\n",
    "                summ_b += user_ratings_train[user][i] - (alpha + item_biases[itemsPerUser[user][i]])\n",
    "            beta_u = (summ_b) / (l+len(user_ratings_train[user]))\n",
    "            user_bias_new[user] = beta_u\n",
    "            user_biases[user] = beta_u\n",
    "        for book in usersPerItem:\n",
    "            summ_b1 = 0\n",
    "            for i in range(len(usersPerItem[book])):\n",
    "                summ_b1 += items_ratings_train[book][i] - (alpha + user_biases[usersPerItem[book][i]])\n",
    "            beta_i = (summ_b1) / (l+len(items_ratings_train[book]))\n",
    "            item_bias_new[book] = beta_i \n",
    "            item_biases[book] = beta_i\n",
    "\n",
    "        for user, book in X_train:\n",
    "            prediction = pred(alpha, item_bias_new[book], user_bias_new[user])\n",
    "            y_pred.append(prediction)\n",
    "        mse = MSE(y_train, y_pred)\n",
    "        counter = counter - 1\n",
    "\n",
    "    y_pred = []\n",
    "    for user, item in X_val:\n",
    "        if user_bias_new[user] != [] and item_bias_new[item] != []:\n",
    "            predictions = pred(alpha, user_bias_new[user], item_bias_new[item])\n",
    "            y_pred.append(predictions)\n",
    "        elif user_bias_new[user] == []:\n",
    "            predictions = pred(alpha, 0, item_bias_new[item])\n",
    "            y_pred.append(predictions)\n",
    "        elif item_bias_new[item] == []:\n",
    "            predictions = pred(alpha, user_bias_new[user], 0)\n",
    "            y_pred.append(predictions)\n",
    "    mse = MSE(y_val, y_pred)\n",
    "    print(f\"The mean squared error on the validation set is: {mse}\")\n",
    "    mse_list.append(mse)\n",
    "    lambda_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f59f21c1908>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f07a3a20>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f07a3278>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f21da748>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f21da0f0>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f39f5cc0>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f04f55c0>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f3a0d978>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f4389978>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f21da9b0>,\n",
       "  <matplotlib.axis.XTick at 0x7f59efdcde48>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f3af96a0>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f3af9f60>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f3af9d30>,\n",
       "  <matplotlib.axis.XTick at 0x7f59efdddba8>,\n",
       "  <matplotlib.axis.XTick at 0x7f59efdddc88>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f3af7550>,\n",
       "  <matplotlib.axis.XTick at 0x7f59efddd048>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f3af9320>],\n",
       " <a list of 19 Text xticklabel objects>)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse_lambda = pd.DataFrame(list(zip(mse_list, lambda_list)), columns=['MSE', 'Lambda'])\n",
    "\n",
    "plt.plot('Lambda', 'MSE', data=mse_lambda)\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Validation MSE')\n",
    "plt.xticks(range(1, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 10\n",
    "user_bias_new = defaultdict(list)\n",
    "item_bias_new = defaultdict(list)\n",
    "old_user_bias = defaultdict(list)\n",
    "old_item_bias = defaultdict(list)\n",
    "counter = 10\n",
    "while counter > 0:\n",
    "    summ = 0 \n",
    "    for x in range(len(X)):\n",
    "        summ += y[x] - (user_biases[X[x][0]]+item_biases[X[x][1]])\n",
    "    alpha = summ / len(y)\n",
    "    y_pred = []     \n",
    "    for user in itemsPerUser:\n",
    "        summ_b = 0\n",
    "        for i in range(len(itemsPerUser[user])):\n",
    "            summ_b += user_ratings_train[user][i] - (alpha + item_biases[itemsPerUser[user][i]])\n",
    "        beta_u = (summ_b) / (2+len(user_ratings_train[user]))\n",
    "        user_bias_new[user] = beta_u\n",
    "        user_biases[user] = beta_u\n",
    "    for book in usersPerItem:\n",
    "        summ_b1 = 0\n",
    "        for i in range(len(usersPerItem[book])):\n",
    "            summ_b1 += items_ratings_train[book][i] - (alpha + user_biases[usersPerItem[book][i]])\n",
    "        beta_i = (summ_b1) / (2+len(items_ratings_train[book]))\n",
    "        item_bias_new[book] = beta_i \n",
    "        item_biases[book] = beta_i\n",
    "\n",
    "    for user, book in X:\n",
    "        prediction = pred(alpha, item_bias_new[book], user_bias_new[user])\n",
    "        y_pred.append(prediction)\n",
    "    mse = MSE(y, y_pred)\n",
    "    counter = counter - 1\n",
    "\n",
    "predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    if user_bias_new[u] != [] and item_bias_new[b] != []:\n",
    "        predictions.write(u + '-' + b + ',' + str(pred(alpha, user_bias_new[u], item_bias_new[b])) + '\\n')\n",
    "    elif user_bias_new[u] == []:\n",
    "        predictions.write(u + '-' + b + ',' + str(pred(alpha, 0, item_bias_new[b])) + '\\n')\n",
    "    elif item_bias_new[b] == []:\n",
    "        predictions.write(u + '-' + b + ',' + str(pred(alpha, user_bias_new[u], 0)) + '\\n')\n",
    "    elif user_bias_new[u] == [] and item_bias_new[b] == []:\n",
    "        predictions.write(u + '-' + b + ',' + str(pred(alpha, 0, 0)) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from statistics import mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "itemsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(list)\n",
    "user_ratings_train = defaultdict(list)\n",
    "user_averages_train = defaultdict(dict)\n",
    "items_ratings_train = defaultdict(list)\n",
    "items_averages_train = defaultdict(dict)\n",
    "user_biases = defaultdict(list)\n",
    "item_biases = defaultdict(list)\n",
    "\n",
    "for user,book,r in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    X.extend([[user, book]])\n",
    "    r = int(r)\n",
    "    y.append(r)\n",
    "    \n",
    "X_train = X[:190000]\n",
    "y_train = y[:190000]\n",
    "X_val = X[190000:200000]\n",
    "y_val = y[190000:200000]\n",
    "\n",
    "for x in range(len(X)):\n",
    "    itemsPerUser[X[x][0]].append(X[x][1])\n",
    "    usersPerItem[X[x][1]].append(X[x][0])\n",
    "    user_ratings_train[X[x][0]].append(y[x])\n",
    "    items_ratings_train[X[x][1]].append(y[x])\n",
    "\n",
    "for u in user_ratings_train:\n",
    "    user_averages_train[u] = sum(user_ratings_train[u]) / len(user_ratings_train[u])\n",
    "    \n",
    "for i in items_ratings_train:\n",
    "    items_averages_train[i] = sum(items_ratings_train[i]) / len(items_ratings_train[i])\n",
    "    \n",
    "global_avg = mean(y_train)\n",
    "\n",
    "for u in user_averages_train:\n",
    "    user_biases[u] = user_averages_train[u] - global_avg\n",
    "for i in items_averages_train:\n",
    "    item_biases[i] = items_averages_train[i] - global_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error on the validation set is: 0.9166657051478027\n",
      "The mean squared error on the validation set is: 0.9230717629220657\n",
      "The mean squared error on the validation set is: 0.9310654940806214\n",
      "The mean squared error on the validation set is: 0.9399199754784744\n",
      "The mean squared error on the validation set is: 0.9492118718702502\n",
      "The mean squared error on the validation set is: 0.9586781622171168\n",
      "The mean squared error on the validation set is: 0.9681523142457509\n",
      "The mean squared error on the validation set is: 0.9775288317196983\n",
      "The mean squared error on the validation set is: 0.9867411931475288\n",
      "The mean squared error on the validation set is: 0.9957480068949741\n",
      "The mean squared error on the validation set is: 1.0045244236801245\n",
      "The mean squared error on the validation set is: 1.013056686911357\n",
      "The mean squared error on the validation set is: 1.0213385580672476\n",
      "The mean squared error on the validation set is: 1.0293689129077594\n",
      "The mean squared error on the validation set is: 1.0371500972360075\n",
      "The mean squared error on the validation set is: 1.0446867880517998\n",
      "The mean squared error on the validation set is: 1.051985197270565\n",
      "The mean squared error on the validation set is: 1.059052511180984\n",
      "The mean squared error on the validation set is: 1.06589649422338\n"
     ]
    }
   ],
   "source": [
    "mse_list = []\n",
    "lambda_list = []\n",
    "\n",
    "for l in range(1, 20):\n",
    "    counter = 10\n",
    "    user_bias_new = defaultdict(list)\n",
    "    item_bias_new = defaultdict(list)\n",
    "    old_user_bias = defaultdict(list)\n",
    "    old_item_bias = defaultdict(list)\n",
    "    counter = 10\n",
    "    while counter > 0:\n",
    "        summ = 0 \n",
    "        for x in range(len(X)):\n",
    "            summ += y[x] - (user_biases[X[x][0]]+item_biases[X[x][1]])\n",
    "        alpha = summ / len(y)\n",
    "        y_pred = []     \n",
    "        for user in itemsPerUser:\n",
    "            summ_b = 0\n",
    "            for i in range(len(itemsPerUser[user])):\n",
    "                summ_b += user_ratings_train[user][i] - (alpha + item_biases[itemsPerUser[user][i]])\n",
    "            beta_u = (summ_b) / (l+len(user_ratings_train[user]))\n",
    "            user_bias_new[user] = beta_u\n",
    "            user_biases[user] = beta_u\n",
    "        for book in usersPerItem:\n",
    "            summ_b1 = 0\n",
    "            for i in range(len(usersPerItem[book])):\n",
    "                summ_b1 += items_ratings_train[book][i] - (alpha + user_biases[usersPerItem[book][i]])\n",
    "            beta_i = (summ_b1) / (l+len(items_ratings_train[book]))\n",
    "            item_bias_new[book] = beta_i \n",
    "            item_biases[book] = beta_i\n",
    "\n",
    "        for user, book in X:\n",
    "            prediction = pred(alpha, item_bias_new[book], user_bias_new[user])\n",
    "            y_pred.append(prediction)\n",
    "        mse = MSE(y, y_pred)\n",
    "        counter = counter - 1\n",
    "\n",
    "    y_pred = []\n",
    "    for user, item in X_val:\n",
    "        if user_bias_new[user] != [] and item_bias_new[item] != []:\n",
    "            predictions = pred(alpha, user_bias_new[user], item_bias_new[item])\n",
    "            y_pred.append(predictions)\n",
    "        elif user_bias_new[user] == []:\n",
    "            predictions = pred(alpha, 0, item_bias_new[item])\n",
    "            y_pred.append(predictions)\n",
    "        elif item_bias_new[item] == []:\n",
    "            predictions = pred(alpha, user_bias_new[user], 0)\n",
    "            y_pred.append(predictions)\n",
    "    mse = MSE(y_val, y_pred)\n",
    "    print(f\"The mean squared error on the validation set is: {mse}\")\n",
    "    mse_list.append(mse)\n",
    "    lambda_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 10\n",
    "user_bias_new = defaultdict(list)\n",
    "item_bias_new = defaultdict(list)\n",
    "old_user_bias = defaultdict(list)\n",
    "old_item_bias = defaultdict(list)\n",
    "counter = 10\n",
    "while counter > 0:\n",
    "    summ = 0 \n",
    "    for x in range(len(X)):\n",
    "        summ += y[x] - (user_biases[X[x][0]]+item_biases[X[x][1]])\n",
    "    alpha = summ / len(y)\n",
    "    y_pred = []     \n",
    "    for user in itemsPerUser:\n",
    "        summ_b = 0\n",
    "        for i in range(len(itemsPerUser[user])):\n",
    "            summ_b += user_ratings_train[user][i] - (alpha + item_biases[itemsPerUser[user][i]])\n",
    "        beta_u = (summ_b) / (1+len(user_ratings_train[user]))\n",
    "        user_bias_new[user] = beta_u\n",
    "        user_biases[user] = beta_u\n",
    "    for book in usersPerItem:\n",
    "        summ_b1 = 0\n",
    "        for i in range(len(usersPerItem[book])):\n",
    "            summ_b1 += items_ratings_train[book][i] - (alpha + user_biases[usersPerItem[book][i]])\n",
    "        beta_i = (summ_b1) / (1+len(items_ratings_train[book]))\n",
    "        item_bias_new[book] = beta_i \n",
    "        item_biases[book] = beta_i\n",
    "\n",
    "    for user, book in X:\n",
    "        prediction = pred(alpha, item_bias_new[book], user_bias_new[user])\n",
    "        y_pred.append(prediction)\n",
    "    mse = MSE(y, y_pred)\n",
    "    counter = counter - 1\n",
    "\n",
    "predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    if user_bias_new[u] != [] and item_bias_new[b] != []:\n",
    "        predictions.write(u + '-' + b + ',' + str(pred(alpha, user_bias_new[u], item_bias_new[b])) + '\\n')\n",
    "    elif user_bias_new[u] == []:\n",
    "        predictions.write(u + '-' + b + ',' + str(pred(alpha, 0, item_bias_new[b])) + '\\n')\n",
    "    elif item_bias_new[b] == []:\n",
    "        predictions.write(u + '-' + b + ',' + str(pred(alpha, user_bias_new[u], 0)) + '\\n')\n",
    "    elif user_bias_new[u] == [] and item_bias_new[b] == []:\n",
    "        predictions.write(u + '-' + b + ',' + str(pred(alpha, 0, 0)) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
