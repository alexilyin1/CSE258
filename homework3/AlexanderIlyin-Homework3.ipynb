{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Alexander Ilyin CSE 258"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import warnings\n",
    "import gzip\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from math import sqrt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def readCSV(path):\n",
    "    f = gzip.open(path, \"rt\")\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        yield l.strip().split(\",\")\n",
    "\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "data = pd.read_csv(\"train_Interactions.csv.gz\")\n",
    "for i in range(len(data)):\n",
    "    X.extend([[data[\"userID\"][i], data[\"bookID\"][i]]])\n",
    "    y.append(1)\n",
    "\n",
    "X_train = X[:190000]\n",
    "y_train = y[:190000]\n",
    "X_val = X[190000:200000]\n",
    "y_val = y[190000:200000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Create \"negative\" entries for books that users haven't read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "all_books = set([x[1] for x in X])\n",
    "\n",
    "for x in X:\n",
    "    u, i = x[0], x[1]\n",
    "    usersPerItem[i].add(u)\n",
    "    itemsPerUser[u].add(i)\n",
    "\n",
    "for x in range(len(X_val)):\n",
    "    user = X_val[x][0]\n",
    "    unread_books = list(all_books.difference(itemsPerUser[user]))\n",
    "    choice = unread_books[random.randint(0, len(unread_books) - 1)]\n",
    "    X_val.extend([[X_val[x][0], choice]])\n",
    "    y_val.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the baseline model is 0.64555\n"
     ]
    }
   ],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for x in range(len(X_train)):\n",
    "    bookCount[X_train[x][1]] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalRead / 2:\n",
    "        break\n",
    "\n",
    "predictions = open(\"predictions_Read_question1.txt\", \"w\")\n",
    "for l in range(len(X_val)):\n",
    "    u = X_val[l][0]\n",
    "    b = X_val[l][1]\n",
    "    if b in return1:\n",
    "        predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "\n",
    "predictions.close()\n",
    "\n",
    "predictions = open(\"predictions_Read_question1.txt\", \"r\")\n",
    "predictions_array = []\n",
    "for l in predictions.readlines():\n",
    "    predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "y_actual = []\n",
    "y_actual = numpy.append(y_actual, y_val)\n",
    "correctPredictions = 0\n",
    "\n",
    "for x in range(len(predictions_array)):\n",
    "    if numpy.array_equal(predictions_array[x], str(int(y_actual[x]))):\n",
    "        correctPredictions = correctPredictions + 1\n",
    "\n",
    "accuracy = correctPredictions / len(predictions_array)\n",
    "print(f\"The accuracy of the baseline model is {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: Improve Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for x in range(len(X)):\n",
    "    bookCount[X[x][1]] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "threshold_list = []\n",
    "accuracy_list = []\n",
    "for threshold in range(0, totalRead, 50000):\n",
    "    return1 = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        return1.add(i)\n",
    "        if count > threshold:\n",
    "            break\n",
    "\n",
    "    predictions = open(\"predictions_Read_question2.txt\", \"w\")\n",
    "    for l in range(len(X_val)):\n",
    "        u = X_val[l][0]\n",
    "        b = X_val[l][1]\n",
    "        if b in return1:\n",
    "            predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "        else:\n",
    "            predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "\n",
    "    predictions.close()\n",
    "\n",
    "    predictions = open(\"predictions_Read_question2.txt\", \"r\")\n",
    "    predictions_array = []\n",
    "    for l in predictions.readlines():\n",
    "        l.strip(\"\\n\")\n",
    "        predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "    y_actual = []\n",
    "    y_actual = numpy.append(y_actual, y_val)\n",
    "    correctPredictions = 0\n",
    "\n",
    "    for x in range(len(predictions_array)):\n",
    "        if predictions_array[x] == str(int(y_actual[x])):\n",
    "            correctPredictions += 1\n",
    "\n",
    "    threshold_list.append(threshold)\n",
    "    accuracy = correctPredictions / len(predictions_array)\n",
    "    accuracy_list.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_threshold = pd.DataFrame(\n",
    "    list(zip(threshold_list, accuracy_list)), columns=[\"threshold\", \"accuracy\"]\n",
    ")\n",
    "plt.plot(\"threshold\", \"accuracy\", data=accuracy_threshold)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "threshold    100000.0000\n",
       "accuracy          0.6541\n",
       "Name: 2, dtype: float64"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_threshold.loc[accuracy_threshold[\"accuracy\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the improved threshold is 0.6596\n"
     ]
    }
   ],
   "source": [
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > 114500:\n",
    "        break\n",
    "\n",
    "predictions = open(\"predictions_Read_question2.txt\", \"w\")\n",
    "for l in range(len(X_val)):\n",
    "    u = X_val[l][0]\n",
    "    b = X_val[l][1]\n",
    "    if b in return1:\n",
    "        predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "\n",
    "predictions.close()\n",
    "\n",
    "predictions = open(\"predictions_Read_question2.txt\", \"r\")\n",
    "predictions_array = []\n",
    "for l in predictions.readlines():\n",
    "    l.strip(\"\\n\")\n",
    "    predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "y_actual = []\n",
    "y_actual = numpy.append(y_actual, y_val)\n",
    "correctPredictions = 0\n",
    "\n",
    "for x in range(len(predictions_array)):\n",
    "    if predictions_array[x] == str(int(y_actual[x])):\n",
    "        correctPredictions += 1\n",
    "\n",
    "accuracy = correctPredictions / len(predictions_array)\n",
    "print(f\"Accuracy for the improved threshold is {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the code snippet above, we can see that the threshold of 114,500 gives us the best accuracy, based on the parameter tuning pipeline created above\n",
    "\n",
    "\n",
    "Note: Does not match the above value, this is because I ran a code that took a very long time to run and simply recorded the answer, I then run a smaller loop to make re-running the code faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for x in range(len(X)):\n",
    "    bookCount[X[x][1]] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > 114500:\n",
    "        break\n",
    "\n",
    "predictions = open(\"predictions_Read_kaggleq1.txt\", \"w\")\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        # header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    if b in return1:\n",
    "        predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "\n",
    "for x in X_train:\n",
    "    u, i = x[0], x[1]\n",
    "    usersPerItem[i].add(u)\n",
    "    itemsPerUser[u].add(i)\n",
    "\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "\n",
    "def mostSimilar(u, b):\n",
    "    similarities = []\n",
    "    books = set(itemsPerUser[u])\n",
    "    for book in books:\n",
    "        sim = Jaccard(usersPerItem[b], usersPerItem[book])\n",
    "        similarities.append((sim, book))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:10]\n",
    "\n",
    "\n",
    "predictions = open(\"predictions_Read_question3.txt\", \"w\")\n",
    "for l in range(len(X_val)):\n",
    "    if mostSimilar(X_val[l][0], X_val[l][1]):\n",
    "        if max(mostSimilar(X_val[l][0], X_val[l][1]))[0] > 0.008:\n",
    "            predictions.write(X_val[l][0] + \"-\" + X_val[l][1] + \",1\\n\")\n",
    "        else:\n",
    "            predictions.write(X_val[l][0] + \"-\" + X_val[l][1] + \",0\\n\")\n",
    "    else:\n",
    "        predictions.write(X_val[l][0] + \"-\" + X_val[l][1] + \",0\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Jaccard similarity baseline model is 0.61945\n"
     ]
    }
   ],
   "source": [
    "predictions = open(\"predictions_Read_question3.txt\", \"r\")\n",
    "predictions_array = []\n",
    "for l in predictions.readlines():\n",
    "    l.strip(\"\\n\")\n",
    "    predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "y_actual = []\n",
    "y_actual = numpy.append(y_actual, y_val)\n",
    "correctPredictions = 0\n",
    "\n",
    "for x in range(len(predictions_array)):\n",
    "    if predictions_array[x] == str(int(y_actual[x])):\n",
    "        correctPredictions += 1\n",
    "\n",
    "accuracy = correctPredictions / len(predictions_array)\n",
    "print(f\"The accuracy of the Jaccard similarity baseline model is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "\n",
    "for x in X_train:\n",
    "    u, i = x[0], x[1]\n",
    "    usersPerItem[i].add(u)\n",
    "    itemsPerUser[u].add(i)\n",
    "\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "\n",
    "def mostSimilar(u, b):\n",
    "    similarities = []\n",
    "    books = set(itemsPerUser[u])\n",
    "    for book in books:\n",
    "        sim = Jaccard(usersPerItem[b], usersPerItem[book])\n",
    "        similarities.append((sim, book))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:10]\n",
    "\n",
    "\n",
    "threshold_list = []\n",
    "accuracy_list = []\n",
    "for threshold in numpy.arange(0, 0.05, 0.01):\n",
    "    predictions = open(\"predictions_Read_question3.txt\", \"w\")\n",
    "    for l in range(len(X_val)):\n",
    "        if mostSimilar(X_val[l][0], X_val[l][1]):\n",
    "            if max(mostSimilar(X_val[l][0], X_val[l][1]))[0] > threshold:\n",
    "                predictions.write(X_val[l][0] + \"-\" + X_val[l][1] + \",1\\n\")\n",
    "            else:\n",
    "                predictions.write(X_val[l][0] + \"-\" + X_val[l][1] + \",0\\n\")\n",
    "        else:\n",
    "            predictions.write(X_val[l][0] + \"-\" + X_val[l][1] + \",0\\n\")\n",
    "    predictions.close()\n",
    "    predictions = open(\"predictions_Read_question3.txt\", \"r\")\n",
    "    predictions_array = []\n",
    "    for l in predictions.readlines():\n",
    "        l.strip(\"\\n\")\n",
    "        predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "    y_actual = []\n",
    "    y_actual = numpy.append(y_actual, y_val)\n",
    "    correctPredictions = 0\n",
    "\n",
    "    for x in range(len(predictions_array)):\n",
    "        if predictions_array[x] == str(int(y_actual[x])):\n",
    "            correctPredictions += 1\n",
    "\n",
    "    threshold_list.append(threshold)\n",
    "    accuracy_list.append(accuracy)\n",
    "    accuracy = correctPredictions / len(predictions_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold_accuracy_jaccard = pd.DataFrame(\n",
    "    list(zip(threshold_list, accuracy_list)), columns=[\"threshold\", \"accuracy\"]\n",
    ")\n",
    "plt.plot(\"threshold\", \"accuracy\", data=threshold_accuracy_jaccard)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "threshold    0.020\n",
       "accuracy     0.625\n",
       "Name: 2, dtype: float64"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_accuracy_jaccard.loc[threshold_accuracy_jaccard[\"accuracy\"].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code chunk above tells us that the Jaccard similarity threshold of .012 gives us the best accuracy. \n",
    "\n",
    "Note that this may not match the printed value above, as I changed the loop to a shorter run time. The value above was calculated from a longer, more exact loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "\n",
    "for x in X_train:\n",
    "    u, i = x[0], x[1]\n",
    "    usersPerItem[i].add(u)\n",
    "    itemsPerUser[u].add(i)\n",
    "\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom\n",
    "\n",
    "\n",
    "def mostSimilar(u, b):\n",
    "    similarities = []\n",
    "    books = set(itemsPerUser[u])\n",
    "    for book in books:\n",
    "        sim = Jaccard(usersPerItem[b], usersPerItem[book])\n",
    "        similarities.append((sim, book))\n",
    "    similarities.sort(reverse=True)\n",
    "    return similarities[:10]\n",
    "\n",
    "\n",
    "predictions = open(\"predictions_Read_kaggleq3.txt\", \"w\")\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        # header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    if mostSimilar(u, b):\n",
    "        if max(mostSimilar(u, b))[0] > 0.0120:\n",
    "            predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "        else:\n",
    "            predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "    else:\n",
    "        predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Jaccard and popularity baseline model is 0.64835\n"
     ]
    }
   ],
   "source": [
    "usersPerItem = defaultdict(set)\n",
    "itemsPerUser = defaultdict(set)\n",
    "\n",
    "for x in X_train:\n",
    "    u, i = x[0], x[1]\n",
    "    usersPerItem[i].add(u)\n",
    "    itemsPerUser[u].add(i)\n",
    "\n",
    "def cosim(set1, set2):\n",
    "    numer = set1.intersection(set2)\n",
    "    numer = len(numer)\n",
    "    denom = len(set1) * len(set2)\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        sim = numer / denom\n",
    "        return(sim)\n",
    "\n",
    "def mostSimilarCosine(u, b):\n",
    "    similarities = []\n",
    "    books = set(itemsPerUser[u])\n",
    "    for book in books:\n",
    "        sim = cosim(set(usersPerItem[b]), set(usersPerItem[book]))\n",
    "        similarities.append(sim)\n",
    "    similarities.sort(reverse=True)\n",
    "    return(similarities)\n",
    "    \n",
    "predictions = open(\"predictions_Read_question4.txt\", \"w\")\n",
    "for l in range(len(X_val)):\n",
    "    u = X_val[l][0]\n",
    "    b = X_val[l][1]\n",
    "    if mostSimilarCosine(u, b):\n",
    "        if b in return1 or mostSimilarCosine(u, b)[0] > 0.005:\n",
    "            predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "        else:\n",
    "            predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "    elif b in return1:\n",
    "        predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "predictions.close()\n",
    "\n",
    "predictions = open(\"predictions_Read_question4.txt\", \"r\")\n",
    "predictions_array = []\n",
    "for l in predictions.readlines():\n",
    "    l.strip(\"\\n\")\n",
    "    predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "y_actual = []\n",
    "y_actual = numpy.append(y_actual, y_val)\n",
    "correctPredictions = 0\n",
    "\n",
    "for x in range(len(predictions_array)):\n",
    "    if predictions_array[x] == str(int(y_actual[x])):\n",
    "        correctPredictions += 1\n",
    "\n",
    "accuracy = correctPredictions / len(predictions_array)\n",
    "print(f\"The accuracy of the Jaccard and popularity baseline model is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosim(set1, set2):\n",
    "    numer = set1.intersection(set2)\n",
    "    numer = len(numer)\n",
    "    denom = len(set1) * len(set2)\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        sim = numer / denom\n",
    "        return(sim)\n",
    "\n",
    "def mostSimilarCosine(u, b):\n",
    "    similarities = []\n",
    "    books = set(itemsPerUser[u])\n",
    "    for book in books:\n",
    "        sim = cosim(set(usersPerItem[b]), set(usersPerItem[book]))\n",
    "        similarities.append(sim)\n",
    "    similarities.sort(reverse=True)\n",
    "    return(similarities)\n",
    "    \n",
    "accuracy_list = []\n",
    "threshold_list = []\n",
    "for threshold_2 in range(100000, 140000, 10000):\n",
    "    for threshold in numpy.arange(0.02, 0.03, 0.001):\n",
    "        return1 = defaultdict(list)\n",
    "        count = 0\n",
    "        for ic, i in mostPopular:\n",
    "            count += ic\n",
    "            return1[i].append(ic)\n",
    "            if count > threshold_2:\n",
    "                break\n",
    "\n",
    "        predictions = open(\"predictions_Read_question4.txt\", \"w\")\n",
    "        for l in range(len(X_val)):\n",
    "            u = X_val[l][0]\n",
    "            b = X_val[l][1]\n",
    "            if mostSimilarCosine(u, b) and len(return1[b]) > 0:\n",
    "                if return1[b][0] * mostSimilarCosine(u, b)[0] > threshold:\n",
    "                    predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "                else:\n",
    "                    predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "            else:\n",
    "                predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "        predictions.close()\n",
    "\n",
    "        predictions = open(\"predictions_Read_question4.txt\", \"r\")\n",
    "        predictions_array = []\n",
    "        for l in predictions.readlines():\n",
    "            l.strip(\"\\n\")\n",
    "            predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "        y_actual = []\n",
    "        y_actual = numpy.append(y_actual, y_val)\n",
    "        correctPredictions = 0\n",
    "\n",
    "        for x in range(len(predictions_array)):\n",
    "            if predictions_array[x] == str(int(y_actual[x])):\n",
    "                correctPredictions += 1\n",
    "\n",
    "        accuracy = correctPredictions / len(predictions_array)\n",
    "        print(f\"The accuracy of the Jaccard and popularity baseline model is {accuracy}\")\n",
    "        accuracy_list.append(accuracy)\n",
    "        threshold_list.append((threshold, threshold_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for x in range(len(X)):\n",
    "    bookCount[X[x][1]] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = defaultdict(list)\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1[i].append(ic)\n",
    "    if count > 120000:\n",
    "        break\n",
    "\n",
    "combos = defaultdict(dict)\n",
    "test_list = []\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    if u == 'userID': continue\n",
    "    if len(return1[b]) > 0:\n",
    "        combos[u][b] = mostSimilarCosine(u,b) * return1[b][0]\n",
    "    else: \n",
    "        combos[u][b] = mostSimilarCosine(u,b)\n",
    "    test_list.append((u, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in combos:\n",
    "    combos[user] = dict(sorted(combos[user].items(), key=lambda x:x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combos[u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b88617544': 0.0,\n",
       " 'b18185034': 0.010869565217391304,\n",
       " 'b57480183': 0.40151515151515155,\n",
       " 'b49930080': 0.5714285714285714}"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combos[u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Read_question4.txt\", \"w\")\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        # header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    if list(combos[u]).index(b) > (len(combos[u]) / 2):\n",
    "        predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Implementing Jaccard and Popularity Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for user, book in X:\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > 120000:\n",
    "        break\n",
    "\n",
    "predictions = open(\"predictions_Read_question4.txt\", \"w\")\n",
    "for l in range(len(X_val)):\n",
    "    u = X_val[l][0]\n",
    "    b = X_val[l][1]\n",
    "    if mostSimilar(X_val[l][0], X_val[l][1]):\n",
    "        if b in return1 or max(mostSimilar(X_val[l][0], X_val[l][1]))[0] > 0.0120:\n",
    "            predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "        else:\n",
    "            predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "    elif b in return1:\n",
    "        predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Jaccard and popularity baseline model is 0.6312\n"
     ]
    }
   ],
   "source": [
    "predictions = open(\"predictions_Read_question4.txt\", \"r\")\n",
    "predictions_array = []\n",
    "for l in predictions.readlines():\n",
    "    l.strip(\"\\n\")\n",
    "    predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "y_actual = []\n",
    "y_actual = numpy.append(y_actual, y_val)\n",
    "correctPredictions = 0\n",
    "\n",
    "for x in range(len(predictions_array)):\n",
    "    if predictions_array[x] == str(int(y_actual[x])):\n",
    "        correctPredictions += 1\n",
    "\n",
    "accuracy = correctPredictions / len(predictions_array)\n",
    "print(f\"The accuracy of the Jaccard and popularity baseline model is {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: Making Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for user, book in X:\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "\n",
    "threshold_popular1_list = []\n",
    "threshold_jaccard1_list = []\n",
    "accuracy1_list = []\n",
    "for threshold_popular1 in range(100000, totalRead, 1000):\n",
    "    for threshold_jaccard1 in numpy.arange(0.001, 0.01, 0.0010):\n",
    "\n",
    "        return1 = set()\n",
    "        count = 0\n",
    "        for ic, i in mostPopular:\n",
    "            count += ic\n",
    "            return1.add(i)\n",
    "            if count > threshold_popular1:\n",
    "                break\n",
    "\n",
    "        predictions = open(\"predictions_Read_question4.txt\", \"w\")\n",
    "        for l in range(len(X_val)):\n",
    "            u = X_val[l][0]\n",
    "            b = X_val[l][1]\n",
    "            if mostSimilar(X_val[l][0], X_val[l][1]):\n",
    "                if (\n",
    "                    b in return1\n",
    "                    and max(mostSimilar(X_val[l][0], X_val[l][1]))[0]\n",
    "                    > threshold_jaccard1\n",
    "                ):\n",
    "                    predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "                else:\n",
    "                    predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "            elif b in return1:\n",
    "                predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "            else:\n",
    "                predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "        predictions.close()\n",
    "\n",
    "        predictions = open(\"predictions_Read_question4.txt\", \"r\")\n",
    "        predictions_array = []\n",
    "        for l in predictions.readlines():\n",
    "            l.strip(\"\\n\")\n",
    "            predictions_array = numpy.append(predictions_array, l[20])\n",
    "\n",
    "        y_actual = []\n",
    "        y_actual = numpy.append(y_actual, y_val)\n",
    "        correctPredictions = 0\n",
    "\n",
    "        for x in range(len(predictions_array)):\n",
    "            if predictions_array[x] == str(int(y_actual[x])):\n",
    "                correctPredictions += 1\n",
    "\n",
    "        accuracy = correctPredictions / len(predictions_array)\n",
    "        threshold_popular1_list.append(threshold_popular1)\n",
    "        threshold_jaccard1_list.append(threshold_jaccard1)\n",
    "        accuracy1_list.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_model_df = pd.DataFrame(\n",
    "    list(zip(threshold_popular1_list, threshold_jaccard1_list, accuracy1_list)),\n",
    "    columns=[\"Popularity Threshold\", \"Jaccard Threshold\", \"Accuracy\"],\n",
    ")\n",
    "combo_model_df.loc[combo_model_df[\"Accuracy\"].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to above, I changed the length of the loop in order to reduce reporducing runtime. The values I chose for my final solution are not represented above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for x in range(len(X)):\n",
    "    bookCount[X[x][1]] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "accuracy_list = []\n",
    "threshold_popular_list = []\n",
    "threshold_jaccard_list = []\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > 132000:\n",
    "        break\n",
    "\n",
    "predictions = open(\"predictions_Read_kaggleq5.txt\", \"w\")\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        # header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    if mostSimilar(u, b):\n",
    "        if b in return1 and max(mostSimilar(u, b))[0] > 0.0080:\n",
    "            predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "        else:\n",
    "            predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "    elif b in return1:\n",
    "        predictions.write(u + \"-\" + b + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + \"-\" + b + \",0\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Kaggle username/display name is \"Alexander Ilyin\",  I signed up with email \"ailyin@ucsd.edu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My top score was .67783, this was using model with jaccard threshold .00750 and popularity threshold of 120,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Fitting Simple Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from statistics import mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "itemsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(list)\n",
    "user_ratings_train = defaultdict(list)\n",
    "user_averages_train = defaultdict(dict)\n",
    "items_ratings_train = defaultdict(list)\n",
    "items_averages_train = defaultdict(dict)\n",
    "user_biases = defaultdict(list)\n",
    "item_biases = defaultdict(list)\n",
    "\n",
    "for user, book, r in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    X.extend([[user, book]])\n",
    "    r = int(r)\n",
    "    y.append(r)\n",
    "\n",
    "X_train = X[:190000]\n",
    "y_train = y[:190000]\n",
    "X_val = X[190000:200000]\n",
    "y_val = y[190000:200000]\n",
    "\n",
    "for x in range(len(X_train)):\n",
    "    itemsPerUser[X_train[x][0]].append(X_train[x][1])\n",
    "    usersPerItem[X_train[x][1]].append(X_train[x][0])\n",
    "    user_ratings_train[X_train[x][0]].append(y_train[x])\n",
    "    items_ratings_train[X_train[x][1]].append(y_train[x])\n",
    "\n",
    "for u in user_ratings_train:\n",
    "    user_averages_train[u] = sum(user_ratings_train[u]) / len(user_ratings_train[u])\n",
    "\n",
    "for i in items_ratings_train:\n",
    "    items_averages_train[i] = sum(items_ratings_train[i]) / len(items_ratings_train[i])\n",
    "\n",
    "global_avg = mean(y_train)\n",
    "\n",
    "for u in user_averages_train:\n",
    "    user_biases[u] = user_averages_train[u] - global_avg\n",
    "for i in items_averages_train:\n",
    "    item_biases[i] = items_averages_train[i] - global_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error on the training set is: 0.9104160042114058\n",
      "The mean squared error on the training set is: 0.9049886289218022\n",
      "The mean squared error on the training set is: 0.9043051229347089\n",
      "The mean squared error on the training set is: 0.9041225817391052\n",
      "The mean squared error on the training set is: 0.9040549183196855\n",
      "The mean squared error on the training set is: 0.904024893265353\n",
      "The mean squared error on the training set is: 0.9040099508579275\n",
      "The mean squared error on the training set is: 0.9040018651649269\n",
      "The mean squared error on the training set is: 0.9039971584267795\n",
      "The mean squared error on the training set is: 0.9039942058988707\n",
      "The mean squared error on the validation set is: 1.1158732273490921\n"
     ]
    }
   ],
   "source": [
    "def pred(a, b, b1):\n",
    "    return a + b + b1\n",
    "\n",
    "\n",
    "def MSE(actual: list, pred: list):\n",
    "    summ = 0\n",
    "    for x in range(len(actual)):\n",
    "        summ += (float(actual[x] - pred[x])) ** 2\n",
    "    summ = summ / len(actual)\n",
    "    return summ\n",
    "\n",
    "\n",
    "old_user_bias = defaultdict(list)\n",
    "old_item_bias = defaultdict(list)\n",
    "counter = 10\n",
    "while counter > 0:\n",
    "    summ = 0\n",
    "    for x in range(len(X_train)):\n",
    "        summ += y_train[x] - (user_biases[X_train[x][0]] + item_biases[X_train[x][1]])\n",
    "    alpha = summ / len(y_train)\n",
    "    y_pred = []\n",
    "    for user in itemsPerUser:\n",
    "        summ_b = 0\n",
    "        for i in range(len(itemsPerUser[user])):\n",
    "            summ_b += user_ratings_train[user][i] - (\n",
    "                alpha + item_biases[itemsPerUser[user][i]]\n",
    "            )\n",
    "        beta_u = (summ_b) / (1 + len(user_ratings_train[user]))\n",
    "        user_bias_new[user] = beta_u\n",
    "        user_biases[user] = beta_u\n",
    "    for book in usersPerItem:\n",
    "        summ_b1 = 0\n",
    "        for i in range(len(usersPerItem[book])):\n",
    "            summ_b1 += items_ratings_train[book][i] - (\n",
    "                alpha + user_biases[usersPerItem[book][i]]\n",
    "            )\n",
    "        beta_i = (summ_b1) / (1 + len(items_ratings_train[book]))\n",
    "        item_bias_new[book] = beta_i\n",
    "        item_biases[book] = beta_i\n",
    "\n",
    "    for user, book in X_train:\n",
    "        prediction = pred(alpha, item_bias_new[book], user_bias_new[user])\n",
    "        y_pred.append(prediction)\n",
    "    mse = MSE(y_train, y_pred)\n",
    "    print(f\"The mean squared error on the training set is: {mse}\")\n",
    "    counter = counter - 1\n",
    "\n",
    "y_pred = []\n",
    "for user, item in X_val:\n",
    "    if user_bias_new[user] != [] and item_bias_new[item] != []:\n",
    "        predictions = pred(alpha, user_bias_new[user], item_bias_new[item])\n",
    "        y_pred.append(predictions)\n",
    "    elif user_bias_new[user] == []:\n",
    "        predictions = pred(alpha, 0, item_bias_new[item])\n",
    "        y_pred.append(predictions)\n",
    "    elif item_bias_new[item] == []:\n",
    "        predictions = pred(alpha, user_bias_new[user], 0)\n",
    "        y_pred.append(predictions)\n",
    "mse = MSE(y_val, y_pred)\n",
    "print(f\"The mean squared error on the validation set is: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: Largest and Smallest beta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larget user beta = u32162993 and smallest user beta = u48313610\n",
      "Largest item beta = b19925500 and smallest item beta = b84091840\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "print(\n",
    "    f\"Larget user beta = {max(user_bias_new.items(), key=operator.itemgetter(1))[0]} and smallest user beta = {min(user_bias_new.items(), key=operator.itemgetter(1))[0]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Largest item beta = {max(item_bias_new.items(), key=operator.itemgetter(1))[0]} and smallest item beta = {min(item_bias_new.items(), key=operator.itemgetter(1))[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11: Finding an optimal lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_list = []\n",
    "lambda_list = []\n",
    "\n",
    "for l in range(1, 5):\n",
    "    counter = 10\n",
    "    user_bias_new = defaultdict(list)\n",
    "    item_bias_new = defaultdict(list)\n",
    "    old_user_bias = defaultdict(list)\n",
    "    old_item_bias = defaultdict(list)\n",
    "    counter = 10\n",
    "    while counter > 0:\n",
    "        summ = 0\n",
    "        for x in range(len(X_train)):\n",
    "            summ += y_train[x] - (\n",
    "                user_biases[X_train[x][0]] + item_biases[X_train[x][1]]\n",
    "            )\n",
    "        alpha = summ / len(y_train)\n",
    "        y_pred = []\n",
    "        for user in itemsPerUser:\n",
    "            summ_b = 0\n",
    "            for i in range(len(itemsPerUser[user])):\n",
    "                summ_b += user_ratings_train[user][i] - (\n",
    "                    alpha + item_biases[itemsPerUser[user][i]]\n",
    "                )\n",
    "            beta_u = (summ_b) / (l + len(user_ratings_train[user]))\n",
    "            user_bias_new[user] = beta_u\n",
    "            user_biases[user] = beta_u\n",
    "        for book in usersPerItem:\n",
    "            summ_b1 = 0\n",
    "            for i in range(len(usersPerItem[book])):\n",
    "                summ_b1 += items_ratings_train[book][i] - (\n",
    "                    alpha + user_biases[usersPerItem[book][i]]\n",
    "                )\n",
    "            beta_i = (summ_b1) / (l + len(items_ratings_train[book]))\n",
    "            item_bias_new[book] = beta_i\n",
    "            item_biases[book] = beta_i\n",
    "\n",
    "        for user, book in X_train:\n",
    "            prediction = pred(alpha, item_bias_new[book], user_bias_new[user])\n",
    "            y_pred.append(prediction)\n",
    "        mse = MSE(y_train, y_pred)\n",
    "        counter = counter - 1\n",
    "\n",
    "    y_pred = []\n",
    "    for user, item in X_val:\n",
    "        if user_bias_new[user] != [] and item_bias_new[item] != []:\n",
    "            predictions = pred(alpha, user_bias_new[user], item_bias_new[item])\n",
    "            y_pred.append(predictions)\n",
    "        elif user_bias_new[user] == []:\n",
    "            predictions = pred(alpha, 0, item_bias_new[item])\n",
    "            y_pred.append(predictions)\n",
    "        elif item_bias_new[item] == []:\n",
    "            predictions = pred(alpha, user_bias_new[user], 0)\n",
    "            y_pred.append(predictions)\n",
    "    mse = MSE(y_val, y_pred)\n",
    "    mse_list.append(mse)\n",
    "    lambda_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f59ef882128>,\n",
       "  <matplotlib.axis.XTick at 0x7f59ef363d68>,\n",
       "  <matplotlib.axis.XTick at 0x7f59ef363160>,\n",
       "  <matplotlib.axis.XTick at 0x7f59f18c1ba8>],\n",
       " <a list of 4 Text xticklabel objects>)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse_lambda = pd.DataFrame(list(zip(mse_list, lambda_list)), columns=[\"MSE\", \"Lambda\"])\n",
    "\n",
    "plt.plot(\"Lambda\", \"MSE\", data=mse_lambda)\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.ylabel(\"Validation MSE\")\n",
    "plt.xticks(range(1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal lambda seems to be 3 in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 10\n",
    "user_bias_new = defaultdict(list)\n",
    "item_bias_new = defaultdict(list)\n",
    "old_user_bias = defaultdict(list)\n",
    "old_item_bias = defaultdict(list)\n",
    "counter = 10\n",
    "while counter > 0:\n",
    "    summ = 0\n",
    "    for x in range(len(X_train)):\n",
    "        summ += y_train[x] - (user_biases[X_train[x][0]] + item_biases[X_train[x][1]])\n",
    "    alpha = summ / len(y_train)\n",
    "    y_pred = []\n",
    "    for user in itemsPerUser:\n",
    "        summ_b = 0\n",
    "        for i in range(len(itemsPerUser[user])):\n",
    "            summ_b += user_ratings_train[user][i] - (\n",
    "                alpha + item_biases[itemsPerUser[user][i]]\n",
    "            )\n",
    "        beta_u = (summ_b) / (3 + len(user_ratings_train[user]))\n",
    "        user_bias_new[user] = beta_u\n",
    "        user_biases[user] = beta_u\n",
    "    for book in usersPerItem:\n",
    "        summ_b1 = 0\n",
    "        for i in range(len(usersPerItem[book])):\n",
    "            summ_b1 += items_ratings_train[book][i] - (\n",
    "                alpha + user_biases[usersPerItem[book][i]]\n",
    "            )\n",
    "        beta_i = (summ_b1) / (3 + len(items_ratings_train[book]))\n",
    "        item_bias_new[book] = beta_i\n",
    "        item_biases[book] = beta_i\n",
    "\n",
    "    for user, book in X_train:\n",
    "        prediction = pred(alpha, item_bias_new[book], user_bias_new[user])\n",
    "        y_pred.append(prediction)\n",
    "    mse = MSE(y_train, y_pred)\n",
    "    counter = counter - 1\n",
    "\n",
    "predictions = open(\"predictions_Rating.txt\", \"w\")\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        # header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    if user_bias_new[u] != [] and item_bias_new[b] != []:\n",
    "        predictions.write(u + \"-\" + b + \",\" + str(pred(alpha, user_bias_new[u], item_bias_new[b])) + \"\\n\")\n",
    "    elif user_bias_new[u] == []:\n",
    "        predictions.write(u + \"-\" + b + \",\" + str(pred(alpha, 0, item_bias_new[b])) + \"\\n\")\n",
    "    elif item_bias_new[b] == []:\n",
    "        predictions.write(u + \"-\" + b + \",\" + str(pred(alpha, user_bias_new[u], 0)) + \"\\n\")\n",
    "    elif user_bias_new[u] == [] and item_bias_new[b] == []:\n",
    "        predictions.write(u + \"-\" + b + \",\" + str(pred(alpha, 0, 0)) + \"\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way I thought to imporve this initial model, was to use more of the dataset for training. I tried various models, but settled on using the WHOLE dataset for training, and found that this gave me the best score on Kaggle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from statistics import mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "itemsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(list)\n",
    "user_ratings_train = defaultdict(list)\n",
    "user_averages_train = defaultdict(dict)\n",
    "items_ratings_train = defaultdict(list)\n",
    "items_averages_train = defaultdict(dict)\n",
    "user_biases = defaultdict(list)\n",
    "item_biases = defaultdict(list)\n",
    "\n",
    "for user, book, r in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    X.extend([[user, book]])\n",
    "    r = int(r)\n",
    "    y.append(r)\n",
    "\n",
    "X_train = X[:190000]\n",
    "y_train = y[:190000]\n",
    "X_val = X[190000:200000]\n",
    "y_val = y[190000:200000]\n",
    "\n",
    "for x in range(len(X)):\n",
    "    itemsPerUser[X[x][0]].append(X[x][1])\n",
    "    usersPerItem[X[x][1]].append(X[x][0])\n",
    "    user_ratings_train[X[x][0]].append(y[x])\n",
    "    items_ratings_train[X[x][1]].append(y[x])\n",
    "\n",
    "for u in user_ratings_train:\n",
    "    user_averages_train[u] = sum(user_ratings_train[u]) / len(user_ratings_train[u])\n",
    "\n",
    "for i in items_ratings_train:\n",
    "    items_averages_train[i] = sum(items_ratings_train[i]) / len(items_ratings_train[i])\n",
    "\n",
    "global_avg = mean(y_train)\n",
    "\n",
    "for u in user_averages_train:\n",
    "    user_biases[u] = user_averages_train[u] - global_avg\n",
    "for i in items_averages_train:\n",
    "    item_biases[i] = items_averages_train[i] - global_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error on the validation set is: 0.9166657051478027\n",
      "The mean squared error on the validation set is: 0.9230717629220657\n",
      "The mean squared error on the validation set is: 0.9310654940806214\n",
      "The mean squared error on the validation set is: 0.9399199754784744\n",
      "The mean squared error on the validation set is: 0.9492118718702502\n",
      "The mean squared error on the validation set is: 0.9586781622171168\n",
      "The mean squared error on the validation set is: 0.9681523142457509\n"
     ]
    }
   ],
   "source": [
    "mse_list = []\n",
    "lambda_list = []\n",
    "\n",
    "for l in range(1, 8):\n",
    "    counter = 10\n",
    "    user_bias_new = defaultdict(list)\n",
    "    item_bias_new = defaultdict(list)\n",
    "    old_user_bias = defaultdict(list)\n",
    "    old_item_bias = defaultdict(list)\n",
    "    counter = 10\n",
    "    while counter > 0:\n",
    "        summ = 0\n",
    "        for x in range(len(X)):\n",
    "            summ += y[x] - (user_biases[X[x][0]] + item_biases[X[x][1]])\n",
    "        alpha = summ / len(y)\n",
    "        y_pred = []\n",
    "        for user in itemsPerUser:\n",
    "            summ_b = 0\n",
    "            for i in range(len(itemsPerUser[user])):\n",
    "                summ_b += user_ratings_train[user][i] - (\n",
    "                    alpha + item_biases[itemsPerUser[user][i]]\n",
    "                )\n",
    "            beta_u = (summ_b) / (l + len(user_ratings_train[user]))\n",
    "            user_bias_new[user] = beta_u\n",
    "            user_biases[user] = beta_u\n",
    "        for book in usersPerItem:\n",
    "            summ_b1 = 0\n",
    "            for i in range(len(usersPerItem[book])):\n",
    "                summ_b1 += items_ratings_train[book][i] - (\n",
    "                    alpha + user_biases[usersPerItem[book][i]]\n",
    "                )\n",
    "            beta_i = (summ_b1) / (l + len(items_ratings_train[book]))\n",
    "            item_bias_new[book] = beta_i\n",
    "            item_biases[book] = beta_i\n",
    "\n",
    "        for user, book in X:\n",
    "            prediction = pred(alpha, item_bias_new[book], user_bias_new[user])\n",
    "            y_pred.append(prediction)\n",
    "        mse = MSE(y, y_pred)\n",
    "        counter = counter - 1\n",
    "\n",
    "    y_pred = []\n",
    "    for user, item in X_val:\n",
    "        if user_bias_new[user] != [] and item_bias_new[item] != []:\n",
    "            predictions = pred(alpha, user_bias_new[user], item_bias_new[item])\n",
    "            y_pred.append(predictions)\n",
    "        elif user_bias_new[user] == []:\n",
    "            predictions = pred(alpha, 0, item_bias_new[item])\n",
    "            y_pred.append(predictions)\n",
    "        elif item_bias_new[item] == []:\n",
    "            predictions = pred(alpha, user_bias_new[user], 0)\n",
    "            y_pred.append(predictions)\n",
    "    mse = MSE(y_val, y_pred)\n",
    "    print(f\"The mean squared error on the validation set is: {mse}\")\n",
    "    mse_list.append(mse)\n",
    "    lambda_list.append(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the optimal lambda was at 1, so we use the whole dataset as training with lambda=1, and predict..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 10\n",
    "user_bias_new = defaultdict(list)\n",
    "item_bias_new = defaultdict(list)\n",
    "old_user_bias = defaultdict(list)\n",
    "old_item_bias = defaultdict(list)\n",
    "counter = 10\n",
    "while counter > 0:\n",
    "    summ = 0 \n",
    "    for x in range(len(X)):\n",
    "        summ += y[x] - (user_biases[X[x][0]]+item_biases[X[x][1]])\n",
    "    alpha = summ / len(y)\n",
    "    y_pred = []     \n",
    "    for user in itemsPerUser:\n",
    "        summ_b = 0\n",
    "        for i in range(len(itemsPerUser[user])):\n",
    "            summ_b += user_ratings_train[user][i] - (alpha + item_biases[itemsPerUser[user][i]])\n",
    "        beta_u = (summ_b) / (1+len(user_ratings_train[user]))\n",
    "        user_bias_new[user] = beta_u\n",
    "        user_biases[user] = beta_u\n",
    "    for book in usersPerItem:\n",
    "        summ_b1 = 0\n",
    "        for i in range(len(usersPerItem[book])):\n",
    "            summ_b1 += items_ratings_train[book][i] - (alpha + user_biases[usersPerItem[book][i]])\n",
    "        beta_i = (summ_b1) / (1+len(items_ratings_train[book]))\n",
    "        item_bias_new[book] = beta_i \n",
    "        item_biases[book] = beta_i\n",
    "\n",
    "    for user, book in X:\n",
    "        prediction = pred(alpha, item_bias_new[book], user_bias_new[user])\n",
    "        y_pred.append(prediction)\n",
    "    mse = MSE(y, y_pred)\n",
    "    counter = counter - 1\n",
    "\n",
    "predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "for l in open(\"pairs_Rating.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        #header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split('-')\n",
    "    if user_bias_new[u] != [] and item_bias_new[b] != []:\n",
    "        predictions.write(u + '-' + b + ',' + str(pred(alpha, user_bias_new[u], item_bias_new[b])) + '\\n')\n",
    "    elif user_bias_new[u] == []:\n",
    "        predictions.write(u + '-' + b + ',' + str(pred(alpha, 0, item_bias_new[b])) + '\\n')\n",
    "    elif item_bias_new[b] == []:\n",
    "        predictions.write(u + '-' + b + ',' + str(pred(alpha, user_bias_new[u], 0)) + '\\n')\n",
    "    elif user_bias_new[u] == [] and item_bias_new[b] == []:\n",
    "        predictions.write(u + '-' + b + ',' + str(pred(alpha, 0, 0)) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import warnings\n",
    "import gzip\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from math import sqrt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "data = pd.read_csv(\"train_Interactions.csv.gz\")\n",
    "for i in range(len(data)):\n",
    "    X.extend([[data[\"userID\"][i], data[\"bookID\"][i]]])\n",
    "    y.append(1)\n",
    "    \n",
    "itemsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(list)\n",
    "user_ratings = defaultdict(list)\n",
    "user_book_ratings = defaultdict(list)\n",
    "user_averages = defaultdict(dict)\n",
    "items_ratings = defaultdict(list)\n",
    "items_averages = defaultdict(dict)\n",
    "all_books = set([x[1] for x in X])\n",
    "\n",
    "\n",
    "def cosim(set1, set2):\n",
    "    numer = set1.intersection(set2)\n",
    "    numer = len(numer)\n",
    "    denom = len(set1) * len(set2)\n",
    "    sim = numer / denom\n",
    "    return(sim)\n",
    "\n",
    "def mostSimilarCosine(u, b):\n",
    "    similarities = []\n",
    "    books = set(itemsPerUser[u])\n",
    "    for book in books:\n",
    "        sim = cosim(set(usersPerItem[b]), set(usersPerItem[book]))\n",
    "        similarities.append(sim)\n",
    "    similarities.sort(reverse=True)\n",
    "    if similarities[0] == 1.0 and len(similarities) > 1:\n",
    "        return similarities[1]\n",
    "    else: \n",
    "        return max(similarities)\n",
    "\n",
    "def distance(set1, set2):\n",
    "    diff1 = set1.difference(set2)\n",
    "    diff2 = set2.difference(set1)\n",
    "    \n",
    "    distance = len(diff1) + len(diff2)\n",
    "    return(-distance)\n",
    "\n",
    "def closest(u, b):\n",
    "    closest = []\n",
    "    books = set(itemsPerUser[u])\n",
    "    for book in books:\n",
    "        dist = distance(set(usersPerItem[u]), set(usersPerItem[book]))\n",
    "        closest.append(dist)\n",
    "    closest.sort(reverse=False)\n",
    "    return closest[:10]\n",
    "\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    numer = len(set(s1).intersection(set(s2)))\n",
    "    denom = len(set(s1).union(set(s2)))\n",
    "    return numer / denom\n",
    "\n",
    "\n",
    "def mostSimilar(u, b):\n",
    "    similarities = []\n",
    "    books = set(itemsPerUser[u])\n",
    "    for book in books:\n",
    "        sim = Jaccard(usersPerItem[b], usersPerItem[book])\n",
    "        similarities.append(sim)\n",
    "    similarities.sort(reverse=True)\n",
    "    if similarities[0] == 1.0 and len(similarities) > 1:\n",
    "        return similarities[1]\n",
    "    else:\n",
    "        return max(similarities)\n",
    "\n",
    "for x in range(len(X)):\n",
    "    itemsPerUser[X[x][0]].append(X[x][1])\n",
    "    usersPerItem[X[x][1]].append(X[x][0])\n",
    "    user_book_ratings[X[x][0], X[x][1]].append(int(y[x]))\n",
    "    user_ratings[X[x][0]].append(int(y[x]))\n",
    "    items_ratings[X[x][1], X[x][1]].append(int(y[x]))\n",
    "    \n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for x in range(len(X)):\n",
    "    bookCount[X[x][1]] += 1\n",
    "    totalRead += 1\n",
    "    \n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:190000]\n",
    "y_train = y[:190000]\n",
    "X_val = X[190000:200000]\n",
    "y_val = y[190000:200000] \n",
    "\n",
    "similarities_list = []\n",
    "count_list = []\n",
    "\n",
    "for x in range(190000):\n",
    "    user = X_train[x][0]\n",
    "    unread_books = list(all_books.difference(itemsPerUser[user]))\n",
    "    choice = unread_books[random.randint(0, len(unread_books) - 1)]\n",
    "    X.extend([[X_train[x][0], choice]])\n",
    "    y.append(0)\n",
    "    \n",
    "for x in range(10000):\n",
    "    user = X_val[x][0]\n",
    "    unread_books = list(all_books.difference(itemsPerUser[user]))\n",
    "    choice = unread_books[random.randint(0, len(unread_books) - 1)]\n",
    "    X.extend([[X_val[x][0], choice]])\n",
    "    y.append(0)\n",
    "\n",
    "for x in range(len(X)):\n",
    "    similarities_list.extend([[1, bookCount[X[x][1]], mostSimilar(X[x][0], X[x][1]), \n",
    "                               mostSimilarCosine(X[x][0], X[x][1])]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_list = []\n",
    "for x in range(len(X)):\n",
    "    joined_list.append((X[x][0], X[x][1], y[x], similarities_list[x][0], similarities_list[x][1], similarities_list[x][2], similarities_list[x][3]))\n",
    "    \n",
    "similarities_df = pd.DataFrame.from_records(joined_list, columns = ['User', 'Book', 'Read', 'Offset', 'Popularity', 'maxJaccard', 'maxCosine'])\n",
    "\n",
    "random.shuffle(joined_list)\n",
    "\n",
    "X_train = []\n",
    "X_val = []\n",
    "y_train = []\n",
    "y_val = []\n",
    "\n",
    "for x in range(380000):\n",
    "    X_train.append((joined_list[x][3], joined_list[x][4], joined_list[x][5], joined_list[x][6]))\n",
    "    y_train.append(joined_list[x][2])\n",
    "    \n",
    "for x in range(380000, 400000):\n",
    "    X_val.append((joined_list[x][3], joined_list[x][4], joined_list[x][5], joined_list[x][6]))\n",
    "    y_val.append(joined_list[x][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012252450490098044\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def find_ber(predictions: list, y: list):\n",
    "    true_positive = 0\n",
    "    false_negative = 0\n",
    "    true_negative = 0\n",
    "    false_positive = 0\n",
    "    balanced_error_rate = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == True and y[i] == True:\n",
    "            true_positive = true_positive + 1\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == False and y[i] == True:\n",
    "            false_negative = false_negative + 1\n",
    "    t_p_r = true_positive / (true_positive + false_negative)\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == False and y[i] == False:\n",
    "            true_negative = true_negative + 1\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == True and y[i] == False:\n",
    "            false_positive = false_positive + 1\n",
    "    t_n_r = true_negative / (true_negative + false_positive)\n",
    "    balanced_error_rate = 1 - (0.5 * (t_p_r + t_n_r))\n",
    "\n",
    "    return balanced_error_rate\n",
    "\n",
    "reg = LogisticRegression(C=1.0)\n",
    "reg.fit(X_train, y_train)\n",
    "pred = reg.predict(X_val)\n",
    "\n",
    "print(find_ber(pred, y_val))\n",
    "correctPredictions = pred == y_val\n",
    "accuracy_test = sum(correctPredictions) / len(correctPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98775"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "lambda_list = []\n",
    "for l in numpy.arange(.1, 10, .5):\n",
    "    reg = LogisticRegression(C=l)\n",
    "    reg.fit(X_train, y_train)\n",
    "    pred = reg.predict(X_val)\n",
    "    \n",
    "    correctPredictions = pred == y_val\n",
    "    accuracy_val = sum(correctPredictions) / len(correctPredictions)\n",
    "    \n",
    "    accuracy_list.append(accuracy_val)\n",
    "    lambda_list.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9736,\n",
       " 0.9836,\n",
       " 0.9859333333333333,\n",
       " 0.9876666666666667,\n",
       " 0.9884,\n",
       " 0.9892,\n",
       " 0.9849666666666667,\n",
       " 0.985,\n",
       " 0.9850666666666666,\n",
       " 0.9851666666666666,\n",
       " 0.9850666666666666,\n",
       " 0.9851,\n",
       " 0.9851666666666666,\n",
       " 0.9851666666666666,\n",
       " 0.9852,\n",
       " 0.9929666666666667,\n",
       " 0.9929666666666667,\n",
       " 0.9930333333333333,\n",
       " 0.9932333333333333,\n",
       " 0.9933666666666666]"
      ]
     },
     "execution_count": 1159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_features = []\n",
    "test_list = []\n",
    "for l in open(\"pairs_Read.txt\"):\n",
    "    u, b = l.strip().split(\"-\")\n",
    "    if u == 'userID': continue\n",
    "    test_list.append((u, b))\n",
    "    pred_features.append((1, bookCount[b], mostSimilar(u, b), \n",
    "                               mostSimilarCosine(u, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LogisticRegression(C=10)\n",
    "reg.fit(X_train, y_train)\n",
    "kaggle_pred = reg.predict(pred_features)\n",
    "made_predictions = defaultdict(list)\n",
    "predictions = open(\"predictions_Read_postdue.txt\", \"w\")\n",
    "predictions.write('userID' + \"-\" + 'bookID,prediction' + '\\n')\n",
    "for x in range(0, len(test_list)):\n",
    "    if sum(made_predictions[test_list[x][0]]) == 0:\n",
    "        predictions.write(test_list[x][0] + \"-\" + test_list[x][1] + ',' + \"1\" + \"\\n\")\n",
    "        made_predictions[test_list[x][0]].append(1)\n",
    "    elif sum(made_predictions[test_list[x][0]]) > 1:\n",
    "        predictions.write(test_list[x][0] + \"-\" + test_list[x][1] + ',' + \"0\" + \"\\n\")\n",
    "        made_predictions[test_list[x][0]].append(0)\n",
    "    else:\n",
    "        predictions.write(test_list[x][0] + \"-\" + test_list[x][1] + ',' + str(kaggle_pred[x]) + \"\\n\")\n",
    "        made_predictions[test_list[x][0]].append(kaggle_pred[x])\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_pred = reg.predict(pred_features)\n",
    "predictions = open(\"predictions_Read_postdue.txt\", \"w\")\n",
    "predictions.write('userID' + \"-\" + 'bookID,prediction' + '\\n')\n",
    "for x in range(len(test_list)):\n",
    "    predictions.write(test_list[x][0] + \"-\" + test_list[x][1] + ',' + str(kaggle_pred[x]) + \"\\n\")\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9523809523809523"
      ]
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "200000/210000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 805,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "150/250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240000.0"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".6 * 400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "itemsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(list)\n",
    "user_ratings = defaultdict(list)\n",
    "user_averages = defaultdict(dict)\n",
    "items_ratings = defaultdict(list)\n",
    "items_averages = defaultdict(dict)\n",
    "user_biases = defaultdict(list)\n",
    "item_biases = defaultdict(list)\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "\n",
    "for user, book, r in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    X.extend([[user, book]])\n",
    "    r = int(r)\n",
    "    y.append(r)\n",
    "\n",
    "X_train = X[:190000]\n",
    "y_train = y[:190000]\n",
    "X_val = X[190000:200000]\n",
    "y_val = y[190000:200000]\n",
    "\n",
    "for x in range(len(X)):\n",
    "    itemsPerUser[X[x][0]].append(X[x][1])\n",
    "    usersPerItem[X[x][1]].append(X[x][0])\n",
    "    user_ratings[X[x][0]].append(y[x])\n",
    "    items_ratings[X[x][1]].append(y[x])\n",
    "\n",
    "for u in user_ratings:\n",
    "    user_averages[u] = sum(user_ratings[u]) / len(user_ratings[u])\n",
    "\n",
    "for i in items_ratings:\n",
    "    items_averages[i] = sum(items_ratings[i]) / len(items_ratings[i])\n",
    "\n",
    "global_avg = mean(y_train)\n",
    "alpha = global_avg\n",
    "    \n",
    "for x in range(len(X)):\n",
    "    user, item, rating = X[x][0], X[x][1], y[x]\n",
    "    reviewsPerUser[user].append((item, rating))\n",
    "    reviewsPerItem[item].append((item, rating))\n",
    "\n",
    "N = len(X)\n",
    "nUsers = len(reviewsPerUser)\n",
    "nItems = len(reviewsPerItem)\n",
    "users = list(reviewsPerUser.keys())\n",
    "items = list(reviewsPerItem.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [],
   "source": [
    "userGamma = {}\n",
    "itemGamma = {}\n",
    "\n",
    "K = 2 \n",
    "\n",
    "for u in reviewsPerUser:\n",
    "    userGamma[u] = [random.random() * 0.1 - 0.05 for k in range(K)]\n",
    "    \n",
    "for i in reviewsPerItem:\n",
    "    itemGamma[i] = [random.random() * 0.1 - 0.05 for k in range(K)]\n",
    "    \n",
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    global userGamma\n",
    "    global itemGamma\n",
    "    index = 0\n",
    "    alpha = theta[index]\n",
    "    index += 1\n",
    "    userBiases = dict(zip(users, theta[index:index+nUsers]))\n",
    "    index += nUsers\n",
    "    itemBiases = dict(zip(items, theta[index:index+nItems]))\n",
    "    index += nItems\n",
    "    for u in users:\n",
    "        userGamma[u] = theta[index:index+K]\n",
    "        index += K\n",
    "    for i in items:\n",
    "        itemGamma[i] = theta[index:index+K]\n",
    "        index += K\n",
    "\n",
    "def MSE_new(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)\n",
    "\n",
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item] + np.inner(userGamma[user], itemGamma[item])\n",
    "\n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(X[x][0], X[x][1]) for x in range(len(X))]\n",
    "    cost = MSE_new(predictions, labels)\n",
    "    print(\"MSE = \" + str(cost))\n",
    "    for u in users:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "        for k in range(K):\n",
    "            cost += lamb*userGamma[u][k]**2\n",
    "    for i in items:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "        for k in range(K):\n",
    "            cost += lamb*itemGamma[i][k]**2\n",
    "    return cost\n",
    "\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(X)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    dUserGamma = {}\n",
    "    dItemGamma = {}\n",
    "    for u in reviewsPerUser:\n",
    "        dUserGamma[u] = [0.0 for k in range(K)]\n",
    "    for i in reviewsPerItem:\n",
    "        dItemGamma[i] = [0.0 for k in range(K)]\n",
    "    for x in range(len(X)):\n",
    "        u, i = X[x][0], X[x][1]\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - y[x]\n",
    "        dalpha += 2/N * diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[i] += 2/N*diff\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2/N*itemGamma[i][k]*diff\n",
    "            dItemGamma[i][k] += 2/N*userGamma[u][k]*diff\n",
    "        for u in userBiases:\n",
    "            dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "        for k in range(K):\n",
    "            dUserGamma[u][k] += 2*lamb*userGamma[u][k]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "        for k in range(K):\n",
    "            dItemGamma[i][k] += 2*lamb*itemGamma[i][k]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    for u in users:\n",
    "        dtheta += dUserGamma[u]\n",
    "    for i in items:\n",
    "        dtheta += dItemGamma[i]\n",
    "    return numpy.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 1.4744191177285175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems) + [random.random() * 0.1 - 0.05 for k in range(K*(nUsers+nItems))],\n",
    "                             derivative, args = (y, 0.001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-934-dde607ddc995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtheta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'theta' is not defined"
     ]
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
